{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[E-04]SongWriter.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cafo4it6hM7H"
      },
      "source": [
        "##1. 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9HRTGlSh-ar"
      },
      "source": [
        "from google.colab import drive\n",
        "import glob\n",
        "import os, re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl1fBHO7vx7V",
        "outputId": "fce55bcb-ddbf-4da4-a1e5-74b7ffd6f6c6"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD6wo7omxk2_"
      },
      "source": [
        "txt_file_path = '/content/drive/MyDrive/Colab Notebooks/lyricist/lyrics/*'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Ojv1LXx9M0"
      },
      "source": [
        "txt_list = glob.glob(txt_file_path)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnlwPcP7yF6j",
        "outputId": "54d00298-1dcf-44e9-97a3-21d6446c1827"
      },
      "source": [
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['Build your dreams to the stars above', 'But when you need someone to love', \"Don't go to strangers, darling, come to me Play with fire till your fingers burn\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uT-L94jATDB"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0a8mxP-AOiD"
      },
      "source": [
        "##2.데이터 정제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8Hbd6OAC6tV"
      },
      "source": [
        "소스 문장(Source Sentence) 및 타겟 문장(Target Sentence)준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ir76T6fB7Lw",
        "outputId": "7d7000df-50be-4ebd-d188-629d36edc963"
      },
      "source": [
        "# 입력된 문장을\n",
        "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
        "#     2. 특수문자 양쪽에 공백을 넣고\n",
        "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
        "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
        "#     5. 다시 양쪽 공백을 지웁니다\n",
        "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 1\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿,),()]+\", \" \", sentence) # 4\n",
        "    sentence = sentence.strip() # 5\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
        "    return sentence\n",
        "\n",
        "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        AAA $$ sen-tence.\"))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> this is sample aaa sen tence . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70hYfGRHrITm"
      },
      "source": [
        "필터링 후 정제된 문장 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsR65rQQDfjO",
        "outputId": "15c00c68-8c65-4096-e8ca-fad189de8c8f"
      },
      "source": [
        "# 여기에 정제된 문장을 모을겁니다\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    if len(preprocess_sentence(sentence).split()) > 15: continue\n",
        "        \n",
        "    # 정제를 하고 담아주세요\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "        \n",
        "# 정제된 결과를 10개만 확인해보죠\n",
        "corpus[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> build your dreams to the stars above <end>',\n",
              " '<start> but when you need someone to love <end>',\n",
              " '<start> and when there s no place for you to turn <end>',\n",
              " '<start> you ll follow your heart i know <end>',\n",
              " '<start> i ve been through it all , for i m an old hand <end>',\n",
              " '<start> and i ll understand if you go so <end>',\n",
              " '<start> make your mark for your friends to see <end>',\n",
              " '<start> but when you need more than company <end>',\n",
              " '<start> you give your soul so sweetly <end>',\n",
              " '<start> tonight the light of love is in your eyes <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ7L36LsGCaq"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcUWX8A5Drzv"
      },
      "source": [
        "##3.정제된 데이터 토큰화 (데이터 전처리)\n",
        "\n",
        "tf.keras.preprocessing.text.Tokenizer 패키지\n",
        "\n",
        ": 정제된 데이터를 토큰화, 단어 사전(vocabulary 또는 dictionary라고 칭함)생성, 데이터를 숫자로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJBi3oGuDu0S",
        "outputId": "0faa0083-c397-4efa-b1c2-912576e46b30"
      },
      "source": [
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "\n",
        "    # 토큰의 개수가 15개 초과하는 문장을 학습데이터에서 제외\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   2 1366   19 ...    0    0    0]\n",
            " [   2   33   45 ...    0    0    0]\n",
            " [   2    8   45 ...    0    0    0]\n",
            " ...\n",
            " [   2  234    1 ...    0    0    0]\n",
            " [   2   10  500 ...    0    0    0]\n",
            " [   2  123   19 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fcd8a993b50>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueKIjtJG2CY1"
      },
      "source": [
        "단어 사전 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8m1TWGA2A9K",
        "outputId": "1296edc9-f2dc-44bb-cbd8-75d61a37609c"
      },
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : i\n",
            "5 : ,\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_e9FRDWGGOQ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM-1TlB_FoEh"
      },
      "source": [
        "##4.소스와 타겟 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKusHDB8FrlM",
        "outputId": "bbd62a4c-54ee-4228-8171-a8f57b1cb735"
      },
      "source": [
        "src_input = tensor[:, :-1]  \n",
        "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
        "tgt_input = tensor[:, 1:]    \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   2 1366   19  388   10    6  623  678    3    0    0    0    0    0]\n",
            "[1366   19  388   10    6  623  678    3    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5IeHSsbCzdE"
      },
      "source": [
        "학습데이터 개수를 124960 미만으로 설정하기 위해서, 2.데이터정제 과정에서 가사 중 괄호 ), ( 를 특수문자와 함께 제거하였음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUmUKnB58A79",
        "outputId": "bee4f619-8694-4dac-d308-f56c22b1b180"
      },
      "source": [
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, shuffle=True, random_state=15)\n",
        "print(\"Source Train:\", enc_train.shape)\n",
        "print(\"Target Train:\", dec_train.shape)\n",
        "print(\"Source Test:\", enc_val.shape)\n",
        "print(\"Target Test:\", enc_val.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source Train: (124735, 14)\n",
            "Target Train: (124735, 14)\n",
            "Source Test: (31184, 14)\n",
            "Target Test: (31184, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uvJBmvjUjhN"
      },
      "source": [
        "\n",
        "\n",
        "--- 임의 테스트\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-xxh-8TULqV",
        "outputId": "5befdef8-8262-4498-c320-c2ddd84f899f"
      },
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buizdU774Occ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wZAIEBlLJ5U"
      },
      "source": [
        "##5.인공지능 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aviQ_h_ELSMZ"
      },
      "source": [
        "인공지능 모델 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkbe9xgVq6D4"
      },
      "source": [
        "embedding_size와 hidden_size를 변경하면서 2.2수준의 val_loss값 도출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlVEZo1-LNUN"
      },
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 256\n",
        "hidden_size = 2048\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP3LiUUjGNo3"
      },
      "source": [
        "모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq5wrZSohQHZ"
      },
      "source": [
        "1. embedding_size = 256, hidden_size = 2048 ==> val_loss = 2.20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsGpW56RbhWm"
      },
      "source": [
        "2. embedding_size = 256, hidden_size = 4096 ==> val_loss = 2.24"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_OvklA4C_z8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c08991-94ae-4f84-c0a9-2208059e0ab8"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(enc_train, dec_train, epochs=10, batch_size=256, validation_data=(enc_val, dec_val),\n",
        "          verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "488/488 [==============================] - 959s 2s/step - loss: 3.4152 - val_loss: 3.0103\n",
            "Epoch 2/10\n",
            "488/488 [==============================] - 877s 2s/step - loss: 2.8811 - val_loss: 2.8098\n",
            "Epoch 3/10\n",
            "488/488 [==============================] - 864s 2s/step - loss: 2.6316 - val_loss: 2.6631\n",
            "Epoch 4/10\n",
            "488/488 [==============================] - 866s 2s/step - loss: 2.3968 - val_loss: 2.5450\n",
            "Epoch 5/10\n",
            "488/488 [==============================] - 879s 2s/step - loss: 2.1584 - val_loss: 2.4446\n",
            "Epoch 6/10\n",
            "488/488 [==============================] - 877s 2s/step - loss: 1.9192 - val_loss: 2.3576\n",
            "Epoch 7/10\n",
            "488/488 [==============================] - 877s 2s/step - loss: 1.6882 - val_loss: 2.3000\n",
            "Epoch 8/10\n",
            "488/488 [==============================] - 864s 2s/step - loss: 1.4794 - val_loss: 2.2553\n",
            "Epoch 9/10\n",
            "488/488 [==============================] - 881s 2s/step - loss: 1.3035 - val_loss: 2.2366\n",
            "Epoch 10/10\n",
            "488/488 [==============================] - 878s 2s/step - loss: 1.1711 - val_loss: 2.2438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2dec7d55d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goxywz-Nh2qe"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f-NQi4fhuAd"
      },
      "source": [
        "##6. 문장 생성 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvbPUuhkelt_"
      },
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
        "    while True:\n",
        "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
        "\n",
        "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "\n",
        "        # 우리 모델이 <END>를 예측하지 않았거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2VmQLQmwpDa"
      },
      "source": [
        "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "estbCVlxwr0s"
      },
      "source": [
        "'<start> i love you, much more than you'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSV4DMa273ZD"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "## 회고\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6JGDNiI9yrj"
      },
      "source": [
        "\n",
        "\n",
        "*   원하는 loss 값이 나오게 하기 위해 여러 수치들을 변경하며 테스트를 진행한다. 이때, 앞으로는 처음 값부터 최종 변경 값까지 정확히 기록을 하며 진행해야겠다. 변경하면서 loss 값이 더 안 좋아지는 경우에는 이전 값으로 돌아가야하는 상황이 발생할 수 있다는 것을 염두에 두어야 한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qF_hPb0r0IM"
      },
      "source": [
        "*   val_loss 값을 2.2 이하로 낮추기 위하여 시도한 것\n",
        "\n",
        "> 1. embedding_size 변경 : 256 --> 14 --> 256  (maxlen과 동일하게 14를 넣었을 때, loss 값은 더 악화 됨.)\n",
        "\n",
        "\n",
        "> 2. hidden_size 변경 : 1024 --> 2048 --> 3072 --> 4096 --> 2048"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA0C_3_atuYc"
      },
      "source": [
        "\n",
        "\n",
        "*   hidden_size 값이 증가할수록 학습 시간이 길어졌다.\n",
        "*   코랩에서 실행 시 hidden_size 값을 4096으로 했을 때, 학습시간이 거의 2시간이 걸렸다.\n",
        "*   hidden_size 값을 2048로 했을 때 결과값이 가장 좋았기 때문에, 다시 값을 변경하여 학습시켰다.\n",
        "*   물리적 시간의 한계로 LMS 클라우드 주피터노트북으로 코랩과 듀얼로 학습을 시켰다.\n",
        "*   Epoch 9 학습을 하고 있는데, 커널 연결이 끊겨서 1시간이 날라갔다.\n",
        "*   colab GPU가 hidden_size 4096으로 학습한 뒤 GPU 사용이 제한되었다.\n",
        "*   다음부터는 특정 값을 변경하여 테스트할 때, colab을 구글 계정 여러개로 동시에 돌려야겠다.\n",
        "*   loss 값을 낮추기 위해, 혹은 적당한 loss 값을 위해 설정해야 하는 수치에 대한 설명이 노드에서 부족한 듯 하다. 구글링을 통해 따로 학습을 해야겠다.\n",
        "\n"
      ]
    }
  ]
}